#version 460 core

#ifndef _NBL_GLSL_WORKGROUP_SIZE_X_
#error "_NBL_GLSL_WORKGROUP_SIZE_X_ is not defined"
#endif

#ifndef _NBL_GLSL_WORKGROUP_SIZE_Y_
#error "_NBL_GLSL_WORKGROUP_SIZE_Y_ is not defined"
#endif

#ifndef _NBL_GLSL_WORKGROUP_SIZE_Z_
#error "_NBL_GLSL_WORKGROUP_SIZE_Z_ is not defined"
#endif

// layout (local_size_x = _NBL_GLSL_WORKGROUP_SIZE_) in;
layout (local_size_x = _NBL_GLSL_WORKGROUP_SIZE_X_, local_size_y = _NBL_GLSL_WORKGROUP_SIZE_Y_, local_size_z = _NBL_GLSL_WORKGROUP_SIZE_Z_) in;

layout(push_constant) uniform Block
{
	uvec3 inDim;
	uvec3 outDim;

	vec3 negativeSupport;
	vec3 positiveSupport;

	uvec3 windowDim;
	uvec3 phaseCount;
} pc;


#define MAX_SMEM_SIZE (16*1024)

// need to change the type based on formats to not waste smem
shared vec4 scratchShared[MAX_SMEM_SIZE/8];

layout (set = 0, binding = 0) uniform sampler3D inImage;
layout (set = 0, binding = 1, r32f) uniform writeonly image3D outImage;

layout (set = 0, binding = 2, std140, row_major) uniform Weights
{
	// Todo(achal): Think about what to put here as MAX_VALUE
	float data[1024];
} weights;

// takes in input space pixel centers
int getWindowMinCoord(in float p)
{
	return int(ceil( (p-0.5f)-abs(pc.negativeSupport) ));
}

// takes in input space pixel centers
int getWindowMaxCoord(in float p)
{
	return int(floor( (p-0.5f)+abs(pc.positiveSupport) ));
}

void main()
{
	// assuming downscale ALWAYS, for now
	const vec3 scale = vec3(pc.inDim)/vec3(pc.outDim);

	const vec3 outputPixelCenter = (gl_WorkGroupID.xyz + vec3(0.5f, 0.5f, 0.5f))*scale;

	const ivec3 windowMinCoord = ivec3(ceil( outputPixelCenter-vec3(0.5f, 0.5f, 0.5f) - abs(pc.negativeSupport) )); // this can be negative
	const ivec3 inputPixelCoord = windowMinCoord + ivec3(gl_LocalInvocationID.xyz);
	const vec3 inputPixelCenter = vec3(inputPixelCoord) + vec3(0.5f, 0.5f, 0.5f);

	const uvec3 globalWindowIndex = gl_WorkGroupID.xyz;
	const uvec3 windowPhase = globalWindowIndex % pc.phaseCount;
	uvec3 lutIndex;
	lutIndex.x = windowPhase.x*pc.windowDim.x + gl_LocalInvocationID.x;
	lutIndex.y = pc.phaseCount.x*pc.windowDim.x + windowPhase.y*pc.windowDim.y + gl_LocalInvocationID.y;
	lutIndex.z = pc.phaseCount.x*pc.windowDim.x + pc.phaseCount.y*pc.windowDim.y + gl_LocalInvocationID.z;

	const vec3 weight = vec3(weights.data[lutIndex.x], weights.data[lutIndex.y], weights.data[lutIndex.z]);

	// Premultiplying weights together like this can butcher floating point precision, is it a better idea to first multiply on the CPU with double precision and then upload??
	scratchShared[gl_LocalInvocationIndex] = texelFetch(inImage, inputPixelCoord, 0)*weight.x*weight.y*weight.z;
	barrier();

	const uvec3 start = uvec3(gl_LocalInvocationID.z*(pc.windowDim.y*pc.windowDim.x) + gl_LocalInvocationID.y*pc.windowDim.x, gl_LocalInvocationID.z*(pc.windowDim.x*pc.windowDim.y), 0u);
	const uvec3 stride = uvec3(1u, pc.windowDim.x, pc.windowDim.x*pc.windowDim.y);

	// Can get axis via push constants
	for (uint axis = 0u; axis < 3u; ++axis)
	{
		const uint elementsToAdd = pc.windowDim[axis];
		const uint stepCount = uint(ceil(log2(float(elementsToAdd)))); // how many steps it would take for it to add all elements in this window

		for (uint step = 0u; step < stepCount; ++step)
		{
			const uint offset = (1 << step);
			const uint baseIndex = (1 << (step+1))*(gl_LocalInvocationIndex - start[axis]);

			if (baseIndex < elementsToAdd)
			{
				vec4 addend = vec4(0.f);
				if (baseIndex + offset < elementsToAdd)
					addend = scratchShared[start[axis] + (baseIndex+offset)*stride[axis]];

				scratchShared[start[axis] + baseIndex*stride[axis]] += addend;
			}
			barrier();
		}

		barrier();
	}

	// Currently doing only one window per workgroup and I expect the result in scratchShared[0]
	if ((gl_LocalInvocationIndex == 0u))
	{
		imageStore(outImage, ivec3(gl_WorkGroupID.xyz), scratchShared[gl_LocalInvocationIndex]);
	}
}